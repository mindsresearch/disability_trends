{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimiter:\n",
    "    def __init__(self, frequency):\n",
    "        self.frequency = frequency\n",
    "        self.last_time = None\n",
    "    def get(self, url:str) -> requests.models.Response:\n",
    "        if self.last_time is not None:\n",
    "            time.sleep(self.frequency - (time.time() - self.last_time))\n",
    "        response = requests.get(url)\n",
    "        self.last_time = time.time()\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_load_url(soup):\n",
    "    fpps = soup.find_all('faceplate-partial')\n",
    "    for fpp in fpps:\n",
    "        a = fpp.attrs\n",
    "        try:\n",
    "            if 'partial-more-posts' in a['id']:\n",
    "                return fpp.attrs['src']\n",
    "        except KeyError:\n",
    "            pass\n",
    "    raise Exception('No loader found!')\n",
    "\n",
    "def get_content(link:str, r:RateLimiter) -> str:\n",
    "    page = r.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup.find('div', class_='text-neutral-content').text.strip()\n",
    "\n",
    "def extract_data(entry, r:RateLimiter):\n",
    "    a = entry.attrs\n",
    "    link = a['content-href']\n",
    "    return {'idx': a['feedindex'],\n",
    "            'title': a['post-title'],\n",
    "            'link': link,\n",
    "            'votes': a['score'],\n",
    "            'comments': a['comment-count'],\n",
    "            'created': datetime.fromisoformat(a['created-timestamp']),\n",
    "            'content': get_content(link, r)}\n",
    "\n",
    "def get_posts(sub: str, calls: int = 1):\n",
    "    url = f'https://www.reddit.com/r/{sub}/?feedViewType=compactView'\n",
    "    r = RateLimiter(1)\n",
    "    print(f'making request 1/{calls}')\n",
    "    page = r.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    posts = soup.find_all('shreddit-post')\n",
    "    out = []\n",
    "    for post in tqdm(posts, desc='extracting posts', leave=False, unit='post'):\n",
    "        out.append(extract_data(post, r))\n",
    "    for i in range(1, calls):\n",
    "        print(f'making request {i+1}/{calls}')\n",
    "        l = get_load_url(soup)\n",
    "        page = r.get(f'https://www.reddit.com{l}')\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        posts = soup.find_all('shreddit-post')\n",
    "        for post in tqdm(posts, desc='extracting posts', leave=False, unit='post'):\n",
    "            out.append(extract_data(post, r))\n",
    "    return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set RECOVER to True if you already have scraped data from here as CSV\n",
    "# Set RECOVER to False if you want to scrape new data\n",
    "\n",
    "RECOVER = True\n",
    "if RECOVER:\n",
    "    df = pd.read_csv('r-autism.csv')\n",
    "else:\n",
    "    df = get_posts('autism', 10)\n",
    "    df.to_csv('r-autism.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "df['title_sentiment'] = df['title'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "df['content_sentiment'] = df['content'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(stopwords=STOPWORDS).generate(' '.join(df['content']))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[1:]['comments'], df[1:]['title_sentiment'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
